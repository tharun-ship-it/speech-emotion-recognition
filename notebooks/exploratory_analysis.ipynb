{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition - Exploratory Data Analysis\n",
    "\n",
    "**Author:** Tharun Ponnam  \n",
    "**Email:** tharunponnam007@gmail.com  \n",
    "**Dataset:** MSP-Podcast Corpus\n",
    "\n",
    "This notebook explores the MSP-Podcast dataset and demonstrates the feature extraction pipeline for speech emotion recognition.\n",
    "\n",
    "## Contents\n",
    "1. Dataset Overview\n",
    "2. Audio Signal Analysis\n",
    "3. Feature Extraction Visualization\n",
    "4. Class Distribution Analysis\n",
    "5. Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Custom modules\n",
    "from src.data.preprocessing import AudioFeatureExtractor\n",
    "from src.data.augmentation import AudioAugmentor\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview\n",
    "\n",
    "The MSP-Podcast corpus contains naturalistic emotional speech from podcast recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "EMOTION_LABELS = {\n",
    "    0: 'Angry',\n",
    "    1: 'Happy',\n",
    "    2: 'Sad',\n",
    "    3: 'Neutral',\n",
    "    4: 'Fear',\n",
    "    5: 'Disgust',\n",
    "    6: 'Surprise',\n",
    "    7: 'Contempt'\n",
    "}\n",
    "\n",
    "# Colors for each emotion\n",
    "EMOTION_COLORS = {\n",
    "    'Angry': '#FF6B6B',\n",
    "    'Happy': '#4ECDC4',\n",
    "    'Sad': '#6B5B95',\n",
    "    'Neutral': '#88D8B0',\n",
    "    'Fear': '#F7DC6F',\n",
    "    'Disgust': '#A0522D',\n",
    "    'Surprise': '#FF69B4',\n",
    "    'Contempt': '#708090'\n",
    "}\n",
    "\n",
    "print(\"Emotion categories:\")\n",
    "for idx, label in EMOTION_LABELS.items():\n",
    "    print(f\"  {idx}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels file (adjust path as needed)\n",
    "LABELS_PATH = '../data/labels.csv'\n",
    "\n",
    "if os.path.exists(LABELS_PATH):\n",
    "    df = pd.read_csv(LABELS_PATH)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    display(df.head(10))\n",
    "else:\n",
    "    print(\"Note: Labels file not found. Using synthetic data for demonstration.\")\n",
    "    # Create synthetic data for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 90103\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'file_id': [f'MSP-PODCAST_{i:05d}' for i in range(n_samples)],\n",
    "        'emotion': np.random.choice(list(EMOTION_LABELS.keys()), n_samples, \n",
    "                                    p=[0.12, 0.15, 0.10, 0.35, 0.08, 0.05, 0.08, 0.07]),\n",
    "        'duration': np.random.exponential(4.5, n_samples),\n",
    "        'split': np.random.choice(['train', 'val', 'test'], n_samples, p=[0.8, 0.1, 0.1])\n",
    "    })\n",
    "    \n",
    "    print(f\"Created synthetic dataset with {n_samples} samples\")\n",
    "    display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map emotion indices to labels\n",
    "df['emotion_label'] = df['emotion'].map(EMOTION_LABELS)\n",
    "\n",
    "# Overall class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "emotion_counts = df['emotion_label'].value_counts()\n",
    "colors = [EMOTION_COLORS[e] for e in emotion_counts.index]\n",
    "\n",
    "axes[0].bar(emotion_counts.index, emotion_counts.values, color=colors, edgecolor='black')\n",
    "axes[0].set_xlabel('Emotion')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Class Distribution')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add count labels\n",
    "for i, (idx, count) in enumerate(zip(emotion_counts.index, emotion_counts.values)):\n",
    "    axes[0].text(i, count + 500, f'{count:,}', ha='center', fontsize=9)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(emotion_counts.values, labels=emotion_counts.index, colors=colors,\n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Class Distribution (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../assets/screenshots/class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass Statistics:\")\n",
    "print(emotion_counts.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution by split\n",
    "split_dist = df.groupby(['split', 'emotion_label']).size().unstack(fill_value=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "split_dist.plot(kind='bar', ax=ax, color=[EMOTION_COLORS[e] for e in split_dist.columns],\n",
    "                edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Split')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Class Distribution by Data Split')\n",
    "ax.legend(title='Emotion', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../assets/screenshots/split_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Audio Signal Analysis\n",
    "\n",
    "Let's visualize audio signals and their properties for different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic audio samples for demonstration\n",
    "# In practice, you would load real audio files\n",
    "\n",
    "def generate_emotional_audio(emotion, duration=3.0, sr=16000):\n",
    "    \"\"\"Generate synthetic audio with emotion-like characteristics.\"\"\"\n",
    "    t = np.linspace(0, duration, int(sr * duration))\n",
    "    \n",
    "    if emotion == 'Angry':\n",
    "        # Higher pitch, more energy, faster variations\n",
    "        freq = 350\n",
    "        audio = 0.8 * np.sin(2 * np.pi * freq * t)\n",
    "        audio += 0.3 * np.sin(2 * np.pi * freq * 2 * t)\n",
    "        audio *= (1 + 0.5 * np.sin(2 * np.pi * 8 * t))  # Fast modulation\n",
    "    elif emotion == 'Happy':\n",
    "        # Variable pitch, upward inflections\n",
    "        freq = 300 + 50 * np.sin(2 * np.pi * 0.5 * t)\n",
    "        audio = 0.6 * np.sin(2 * np.pi * freq * t)\n",
    "        audio += 0.2 * np.sin(2 * np.pi * freq * 1.5 * t)\n",
    "    elif emotion == 'Sad':\n",
    "        # Lower pitch, slower variations, less energy\n",
    "        freq = 180\n",
    "        audio = 0.4 * np.sin(2 * np.pi * freq * t)\n",
    "        audio *= np.exp(-0.3 * t)  # Decay envelope\n",
    "    elif emotion == 'Neutral':\n",
    "        # Steady, moderate pitch and energy\n",
    "        freq = 220\n",
    "        audio = 0.5 * np.sin(2 * np.pi * freq * t)\n",
    "    else:\n",
    "        freq = 250\n",
    "        audio = 0.5 * np.sin(2 * np.pi * freq * t)\n",
    "    \n",
    "    # Add some noise for realism\n",
    "    audio += 0.05 * np.random.randn(len(audio))\n",
    "    \n",
    "    return audio.astype(np.float32)\n",
    "\n",
    "# Generate samples\n",
    "sample_rate = 16000\n",
    "emotions_to_plot = ['Angry', 'Happy', 'Sad', 'Neutral']\n",
    "audio_samples = {e: generate_emotional_audio(e, sr=sample_rate) for e in emotions_to_plot}\n",
    "\n",
    "print(\"Generated synthetic audio samples for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize waveforms\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (emotion, audio) in enumerate(audio_samples.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    time = np.arange(len(audio)) / sample_rate\n",
    "    ax.plot(time, audio, color=EMOTION_COLORS[emotion], linewidth=0.5)\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.set_title(f'{emotion} - Waveform')\n",
    "    ax.set_xlim([0, 3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../assets/screenshots/waveforms.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize mel spectrograms\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (emotion, audio) in enumerate(audio_samples.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Compute mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=sample_rate, n_mels=128, hop_length=512\n",
    "    )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    img = librosa.display.specshow(\n",
    "        mel_spec_db, sr=sample_rate, hop_length=512,\n",
    "        x_axis='time', y_axis='mel', ax=ax, cmap='magma'\n",
    "    )\n",
    "    ax.set_title(f'{emotion} - Mel Spectrogram')\n",
    "    plt.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../assets/screenshots/mel_spectrograms.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature extractor\n",
    "extractor = AudioFeatureExtractor(\n",
    "    sample_rate=16000,\n",
    "    n_mfcc=40,\n",
    "    n_mels=128,\n",
    "    hop_length=512\n",
    ")\n",
    "\n",
    "print(\"Feature Extractor Configuration:\")\n",
    "print(f\"  Sample Rate: {extractor.sample_rate}\")\n",
    "print(f\"  N_MFCC: {extractor.n_mfcc}\")\n",
    "print(f\"  N_Mels: {extractor.n_mels}\")\n",
    "print(f\"  Hop Length: {extractor.hop_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for each emotion\n",
    "features_dict = {}\n",
    "\n",
    "for emotion, audio in audio_samples.items():\n",
    "    features = extractor.extract(audio)\n",
    "    features_dict[emotion] = features\n",
    "    print(f\"{emotion}: Feature shape = {features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MFCCs\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (emotion, audio) in enumerate(audio_samples.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Compute MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40, hop_length=512)\n",
    "    \n",
    "    img = librosa.display.specshow(\n",
    "        mfccs, sr=sample_rate, hop_length=512,\n",
    "        x_axis='time', ax=ax, cmap='coolwarm'\n",
    "    )\n",
    "    ax.set_title(f'{emotion} - MFCCs')\n",
    "    ax.set_ylabel('MFCC Coefficient')\n",
    "    plt.colorbar(img, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../assets/screenshots/mfccs.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature statistics per emotion\n",
    "stats = []\n",
    "\n",
    "for emotion, features in features_dict.items():\n",
    "    stats.append({\n",
    "        'Emotion': emotion,\n",
    "        'Mean': features.mean(),\n",
    "        'Std': features.std(),\n",
    "        'Min': features.min(),\n",
    "        'Max': features.max(),\n",
    "        'Frames': features.shape[0],\n",
    "        'Features': features.shape[1]\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation analysis\n",
    "# Use neutral audio for demonstration\n",
    "neutral_features = features_dict['Neutral']\n",
    "\n",
    "# Compute correlation matrix for first 20 features\n",
    "corr_matrix = np.corrcoef(neutral_features[:, :20].T)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0,\n",
    "            xticklabels=range(1, 21), yticklabels=range(1, 21))\n",
    "plt.title('Feature Correlation Matrix (First 20 Features)')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Feature Index')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../assets/screenshots/feature_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Augmentation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize augmentor\n",
    "augmentor = AudioAugmentor()\n",
    "\n",
    "# Get a sample audio\n",
    "original_audio = audio_samples['Neutral']\n",
    "\n",
    "# Apply augmentations\n",
    "augmented = {\n",
    "    'Original': original_audio,\n",
    "    'Noise (SNR=20dB)': augmentor.add_noise(original_audio, snr_db=20),\n",
    "    'Time Stretch (1.2x)': augmentor.time_stretch(original_audio, rate=1.2),\n",
    "    'Pitch Shift (+3)': augmentor.pitch_shift(original_audio, sample_rate, n_steps=3)\n",
    "}\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, audio) in enumerate(augmented.items()):\n",
    "    ax = axes[idx]\n",
    "    time = np.arange(len(audio)) / sample_rate\n",
    "    ax.plot(time, audio, linewidth=0.5)\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlim([0, min(3, len(audio)/sample_rate)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../assets/screenshots/augmentation_examples.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Dataset Analysis**: Class distribution shows natural imbalance typical of emotion datasets\n",
    "2. **Audio Visualization**: Waveforms and spectrograms reveal emotion-specific patterns\n",
    "3. **Feature Extraction**: 180-dimensional features combining MFCCs, mel-spectrograms, and prosodic features\n",
    "4. **Data Augmentation**: Various techniques to improve model robustness\n",
    "\n",
    "Key insights:\n",
    "- Neutral emotion dominates (~35%), requiring careful handling of class imbalance\n",
    "- Different emotions show distinct spectral characteristics\n",
    "- Prosodic features (pitch, energy) provide complementary information to spectral features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
